{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f94c635a-972d-402a-9f72-0618aaac746d",
   "metadata": {},
   "source": [
    "# Retrieval & preprocessing AND Natural Language Processing (NLP) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a15a46a-980b-4648-8413-d46489cde4e9",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded82f95-f8e4-4763-ab47-cb9897f0b383",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2e7f9cf0-fc70-40b2-ad1b-2538aba509a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_ollama import OllamaLLM\n",
    "from langchain.schema import Document\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from tqdm import tqdm\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d9de65-9b47-4dad-b72a-f3c576ae4bac",
   "metadata": {},
   "source": [
    "### Load the vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f4fcb17b-3280-4ecf-a9d3-68924d6eecfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading FAISS vector database...\n",
      "FAISS vector database loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Load the FAISS vector database\n",
    "save_path = \"../vector_database\"\n",
    "model_name = \"all-MPNet-base-v2\"\n",
    "\n",
    "print(\"Loading FAISS vector database...\")\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=model_name)\n",
    "loaded_faiss_index = FAISS.load_local(save_path, embedding_model, allow_dangerous_deserialization=True)\n",
    "\n",
    "if loaded_faiss_index:\n",
    "    print(\"FAISS vector database loaded successfully!\")\n",
    "else:\n",
    "    print(\"Something went wrong\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb0ac60",
   "metadata": {},
   "source": [
    "## Retrieval & preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd2b148-0d48-4735-8da1-fed767be3b43",
   "metadata": {},
   "source": [
    "### Filter irrelevant chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "abad74d2-f6ed-405d-90dd-e6a0b0b95c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def filter_irrelevant_chunks(documents, max_dot_ratio=0.5, min_length=30):\n",
    "    \"\"\"\n",
    "    Filter out irrelevant chunks that contain table of contents, excessive dots,\n",
    "    or sections unrelated to SQL content such as abstracts, introductions, conclusions, etc.\n",
    "    \n",
    "    Args:\n",
    "        documents (list): List of retrieved documents (chunks).\n",
    "        max_dot_ratio (float): The maximum allowed ratio of dots to words for a chunk to be considered relevant.\n",
    "        min_chunk_length (int): The minimum length for a chunk to be considered informative.\n",
    "        \n",
    "    Returns:\n",
    "        list: Filtered list of relevant documents.\n",
    "    \"\"\"\n",
    "    relevant_chunks = []\n",
    "\n",
    "    # Define patterns for identifying table of contents, irrelevant sections, and excessive dots\n",
    "    irrelevant_patterns = [\n",
    "        r\"table\\s*des\\s*matières\",  # table of contents\n",
    "        r\"liste\\s*des\\s*(figures|tables)\",  # list of figures or tables\n",
    "        r\"\\.{3,}\",  # More than 3 consecutive dots (likely a table of contents)\n",
    "        r\"\\b(guide|résumé|abstract|remerciement|introduction|conclusion|références|bibliographie|webographie)\\b\",  # Common non-SQL sections\n",
    "    ]\n",
    "\n",
    "    for doc in documents:\n",
    "        # Step 1: Check if the chunk contains any irrelevant pattern\n",
    "        text = doc.page_content.strip().lower()\n",
    "\n",
    "        # If any irrelevant pattern is matched, skip this chunk\n",
    "        if any(re.search(pattern, text) for pattern in irrelevant_patterns):\n",
    "            continue\n",
    "\n",
    "        # Step 2: Check if the chunk has too many dots (likely a table of contents)\n",
    "        dot_ratio = text.count('.') / len(text.split())  # Calculate dot ratio\n",
    "        if dot_ratio > max_dot_ratio:\n",
    "            continue\n",
    "\n",
    "        # Step 3: Check if the chunk is short\n",
    "        # Split the text into words and check length\n",
    "        words = doc.page_content.split()\n",
    "        if len(words) < min_length:\n",
    "            continue\n",
    "\n",
    "        # If the chunk passes all checks, keep it\n",
    "        relevant_chunks.append(doc)\n",
    "\n",
    "    return relevant_chunks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f08c9c52-df24-44c0-b1e2-14cc8fbd59ee",
   "metadata": {},
   "source": [
    "### Retrieve the documents from the vector database based on the user query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0edc9494-0ec7-44b8-818c-475554e39764",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_documents_from_faiss(query, faiss_index, k=5):\n",
    "    \"\"\"\n",
    "    Retrieve the top-k relevant documents for a given query from the FAISS index.\n",
    "\n",
    "    Args:\n",
    "        query (str): The user's query.\n",
    "        faiss_index (FAISS): The FAISS index containing the document embeddings.\n",
    "        k (int): The number of top results to retrieve.\n",
    "\n",
    "    Returns:\n",
    "        list: List of relevant documents.\n",
    "    \"\"\"\n",
    "    retriever = faiss_index.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": k})\n",
    "    retrieved_docs = retriever.get_relevant_documents(query)\n",
    "    return retrieved_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a74245ef-8556-4a7c-b96b-b86b68bcced7",
   "metadata": {},
   "source": [
    "### Re-rank the retrieved documents using a re-ranking model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3470a3e9-4a77-4357-8ef0-51111c9dd646",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "# Load a re-ranker (for simplicity, we use a transformer model for re-ranking)\n",
    "re_ranker = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n",
    "\n",
    "def rerank_documents(query, documents):\n",
    "\n",
    "    re_ranked = []\n",
    "\n",
    "    for doc in documents:\n",
    "        # Use a zero-shot classifier to rank documents based on relevance to the query\n",
    "        result = re_ranker(query, candidate_labels=[doc.page_content])\n",
    "        score = result['scores'][0]  # Take the relevance score\n",
    "\n",
    "        # Store document along with its score\n",
    "        re_ranked.append({\"document\": doc, \"score\": score})\n",
    "\n",
    "    # Sort documents based on score (higher score is more relevant)\n",
    "    re_ranked.sort(key=lambda x: x['score'], reverse=True)\n",
    "\n",
    "    # Return the top re-ranked documents\n",
    "    return [doc[\"document\"] for doc in re_ranked]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf5aa99-f43c-4d94-a693-dc467a7552b9",
   "metadata": {},
   "source": [
    "### Combine all the three previous functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ab2949e3-6be4-4581-a5ad-92be8adba4b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_and_rerank(query, faiss_index, k=25, top_n=5):\n",
    "    \"\"\"\n",
    "    Retrieve and re-rank documents based on their relevance to a query.\n",
    "\n",
    "    Args:\n",
    "        query (str): The user's query in French.\n",
    "        faiss_index (FAISS): The FAISS vector store.\n",
    "        k (int): The number of top results to retrieve (set to 25).\n",
    "        top_n (int): The number of top re-ranked documents to return (set to 5).\n",
    "\n",
    "    Returns:\n",
    "        list: Top re-ranked documents.\n",
    "    \"\"\"\n",
    "    # Step 1: Retrieve top-k relevant documents from FAISS index\n",
    "    retrieved_docs = retrieve_documents_from_faiss(query, faiss_index, k)\n",
    "\n",
    "    # Step 2: Filter out irrelevant documents (e.g., TOC, excessive dots)\n",
    "    filtered_docs = filter_irrelevant_chunks(retrieved_docs)\n",
    "\n",
    "    # Step 3: Re-rank the filtered documents\n",
    "    re_ranked_docs = rerank_documents(query, filtered_docs)\n",
    "\n",
    "    # Return the top 'top_n' re-ranked documents\n",
    "    return re_ranked_docs[:top_n]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3bf92b-c23c-4b38-9cda-be3eb8810d8a",
   "metadata": {},
   "source": [
    "### Clean the re-ranked documents to prepare them for question generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2f79ca00-be74-4d0a-9ad4-6f800498eb77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_chunk_for_question_generation(chunk, min_length):\n",
    "    \"\"\"\n",
    "    Clean the chunk for question generation by removing unnecessary text and ensuring meaningful content.\n",
    "\n",
    "    Args:\n",
    "        chunk (str): The chunk of text to be cleaned.\n",
    "        min_length (int): Minimum length of the chunk to ensure it's meaningful.\n",
    "\n",
    "    Returns:\n",
    "        str: Cleaned chunk ready for question generation.\n",
    "    \"\"\"\n",
    "    # Step 1: Normalize whitespace (remove extra spaces, newlines)\n",
    "    cleaned_text = re.sub(r'[ \\t]+', ' ', chunk).strip()\n",
    "\n",
    "    # Step 2: Remove non-informative filler phrases like \"introduction\", \"summary\"\n",
    "    cleaned_text = re.sub(r'\\b(introduction|résumé|summary|conclusion)\\b', '', cleaned_text, flags=re.IGNORECASE)\n",
    "\n",
    "    # Step 3: Ensure minimum content length for meaningful chunk\n",
    "    if len(cleaned_text.split()) < min_length:\n",
    "        return None  # Chunk is too short to be meaningful for question generation\n",
    "\n",
    "    return cleaned_text\n",
    "\n",
    "def clean_relevant_chunks_for_question_generation(re_ranked_docs, min_chunk_length):\n",
    "    cleaned_chunks = []\n",
    "\n",
    "    for doc in re_ranked_docs:\n",
    "        cleaned_chunk = clean_chunk_for_question_generation(doc.page_content, min_chunk_length)\n",
    "        if cleaned_chunk:  # Only add the chunk if it's not None (meaningful)\n",
    "            cleaned_chunks.append(cleaned_chunk)\n",
    "\n",
    "    return cleaned_chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e4c0d48",
   "metadata": {},
   "source": [
    "## Natural Language Processing (NLP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4cb27a6-5f46-4892-aaae-220af78b88e2",
   "metadata": {},
   "source": [
    "### Define Question Generation functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efc243e2-a02f-4d5c-a4ad-7e51c98ddfa3",
   "metadata": {},
   "source": [
    "#### Generate QCM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fbb5093a-5c47-4b87-b591-821051e99f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_mcq(content, query, difficulty=\"intermédiaire\"):\n",
    "    \"\"\"\n",
    "    Génère des questions à choix multiples (QCM) en français à partir du contenu fourni,\n",
    "    en respectant la requête de l'utilisateur et en tenant compte du niveau de difficulté spécifié.\n",
    "    \n",
    "    Args:\n",
    "        content (str): Le contenu pour générer les questions.\n",
    "        query (str): La requête de l'utilisateur, qui doit guider les questions.\n",
    "        difficulty (str): Le niveau de difficulté ('débutant', 'intermédiaire', 'avancé').\n",
    "\n",
    "    Returns:\n",
    "        dict: Question générée avec les options, la réponse correcte et une explication.\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    Vous êtes un assistant spécialisé dans la génération de questions à choix multiples (QCM) en français.\n",
    "    Utilisez le contenu suivant pour créer une question conforme aux consignes ci-dessous.\n",
    "\n",
    "    ### Consignes :\n",
    "    1. **Lien avec la requête** :\n",
    "       La question doit être liée au concept suivant : '{query}' et tester la compréhension de ce concept.\n",
    "    \n",
    "    2. **Niveau de difficulté** :\n",
    "       Adaptez la complexité de la question au niveau spécifié : '{difficulty}'.\n",
    "    \n",
    "    3. **Structure des options** :\n",
    "       - Créez une question avec quatre options de réponse (A, B, C, D), dont une seule est correcte.\n",
    "       - Assurez-vous que les options sont logiquement distinctes et pertinentes.\n",
    "\n",
    "    4. **Format de sortie** :\n",
    "       Retournez la question sous le format JSON suivant :\n",
    "       ```json\n",
    "       {{\n",
    "           \"question\": \"<La question clairement formulée>\",\n",
    "           \"options\": {{\n",
    "               \"A\": \"<Option A>\",\n",
    "               \"B\": \"<Option B>\",\n",
    "               \"C\": \"<Option C>\",\n",
    "               \"D\": \"<Option D>\"\n",
    "           }},\n",
    "           \"correct_answer\": \"<Lettre de l'option correcte (A, B, C ou D)>\",\n",
    "           \"explanation\": \"<Explication concise et claire de la réponse correcte>\"\n",
    "       }}\n",
    "       ```\n",
    "\n",
    "    5. **Précision et clarté** :\n",
    "       Toutes les informations doivent être extraites uniquement du contenu fourni. Évitez toute ambiguïté.\n",
    "\n",
    "    ### Contenu à utiliser :\n",
    "    {content}\n",
    "    \"\"\"\n",
    "    response = ollama_model.invoke(prompt)\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3987da6-5628-42e1-8385-4dccda7c93f7",
   "metadata": {},
   "source": [
    "#### Generate open-ended questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ca69013d-715c-40c5-97e3-cb4f7fcf2f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_open_ended(content, query, difficulty=\"intermédiaire\"):\n",
    "    \"\"\"\n",
    "    Génère des questions ouvertes en français à partir du contenu fourni,\n",
    "    en veillant à ce qu'elles soient pertinentes par rapport à la requête de l'utilisateur.\n",
    "\n",
    "    Args:\n",
    "        content (str): Le contenu pour générer les questions.\n",
    "        query (str): La requête de l'utilisateur, qui doit guider les questions.\n",
    "        difficulty (str): Le niveau de difficulté ('débutant', 'intermédiaire', 'avancé').\n",
    "\n",
    "    Returns:\n",
    "        dict: Question ouverte générée avec une réponse exemple et une explication.\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    Vous êtes un assistant spécialisé dans la génération de questions ouvertes en français.\n",
    "    Utilisez le contenu ci-dessous pour créer une question ouverte en respectant les consignes suivantes.\n",
    "\n",
    "    ### Consignes :\n",
    "    1. **Lien avec la requête** :\n",
    "       La question doit être basée sur le concept suivant : '{query}' et adaptée au niveau de difficulté '{difficulty}'.\n",
    "\n",
    "    2. **Format de sortie** :\n",
    "       Retournez les résultats sous le format JSON suivant :\n",
    "       ```json\n",
    "       {{\n",
    "           \"question\": \"<La question clairement formulée>\",\n",
    "           \"example_answer\": \"<Une réponse exemple concise>\",\n",
    "           \"explanation\": \"<Explication claire de la réponse>\"\n",
    "       }}\n",
    "       ```\n",
    "\n",
    "    3. **Précision et clarté** :\n",
    "       Toutes les informations doivent provenir uniquement du contenu fourni.\n",
    "\n",
    "    ### Contenu à utiliser :\n",
    "    {content}\n",
    "    \"\"\"\n",
    "    response = ollama_model.invoke(prompt)\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e256b8-3d7e-438e-9808-9d11d0642183",
   "metadata": {},
   "source": [
    "#### Generate Questions peipline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a2d3a1c3-7c7c-4723-b198-ef68d3a6609c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def exam_pipeline(query, question_type, question_nbr, faiss_index, ollama_model, difficulty=\"intermediate\", k=25, top_n=5):\n",
    "#     \"\"\"\n",
    "#     The exam generation pipeline that retrieves documents, cleans them, and generates questions based on the retrieved chunks.\n",
    "    \n",
    "#     Args:\n",
    "#         query (str): The query to search for.\n",
    "#         question_type (str): The type of questions ('mcq' or 'open-ended').\n",
    "#         question_nbr (int): The number of questions to generate.\n",
    "#         faiss_index (FAISS): The FAISS vector store for document retrieval.\n",
    "#         ollama_model (OllamaModel): The model used for question generation (Gemma2).\n",
    "#         difficulty (str): The difficulty level for the questions ('beginner', 'intermediate', 'advanced').\n",
    "#         k (int): The number of relevant documents to retrieve.\n",
    "#         top_n (int): The number of documents to keep after re-ranking.\n",
    "        \n",
    "#     Returns:\n",
    "#         dict: A dictionary containing the generated exam questions.\n",
    "#     \"\"\"\n",
    "    \n",
    "#     # Step 1: Retrieve Documents\n",
    "#     print(\"Retrieving relevant documents...\")\n",
    "#     re_ranked_docs = retrieve_and_rerank(query, faiss_index, k, top_n)\n",
    "\n",
    "#     if not re_ranked_docs:\n",
    "#         print(\"No documents found for the given query.\")\n",
    "#         return {}\n",
    "\n",
    "#     print(f\"Retrieved {len(re_ranked_docs)} documents.\")\n",
    "\n",
    "#     # Step 2: Normalize Retrieved Chunks\n",
    "#     print(\"Cleaning retrieved content...\")\n",
    "#     cleaned_chunks = clean_relevant_chunks_for_question_generation(re_ranked_docs, min_chunk_length=30)\n",
    "\n",
    "#     print(f\"Normalized to {len(cleaned_chunks)} valid chunks.\")\n",
    "\n",
    "#     # Step 3: Generate Questions\n",
    "#     print(\"Generating questions...\")\n",
    "#     exam = {\"questions\": []}\n",
    "#     base = cleaned_chunks  # Base content to generate questions from\n",
    "\n",
    "#     for i in tqdm(range(0, question_nbr), desc=\"Generating Questions\"):\n",
    "#         try:\n",
    "#             # Select content for question generation\n",
    "#             content = base[i % len(base)]  # Loop through base content\n",
    "\n",
    "#             # Generate MCQ or Open-ended question based on type\n",
    "#             if question_type.lower() == \"mcq\":\n",
    "#                 response = generate_mcq(content, query, difficulty)\n",
    "#                 question = {\n",
    "#                     \"type\": \"mcq\",\n",
    "#                     \"source_content\": content,  # Source content for the question\n",
    "#                     \"question_data\": response\n",
    "#                 }\n",
    "#             elif question_type.lower() == \"open-ended\":\n",
    "#                 response = generate_open_ended(content, difficulty)\n",
    "#                 question = {\n",
    "#                     \"type\": \"open-ended\",\n",
    "#                     \"source_content\": content,  # Source content for the question\n",
    "#                     \"question_data\": response\n",
    "#                 }\n",
    "#             else:\n",
    "#                 raise ValueError(\"Invalid question type. Use 'mcq' or 'open-ended'.\")\n",
    "\n",
    "#             exam[\"questions\"].append(question)\n",
    "\n",
    "#         except Exception as e:\n",
    "#             print(f\"Error generating question: {e}\")\n",
    "\n",
    "#     print(f\"Generated {len(exam['questions'])} questions.\")\n",
    "\n",
    "#     return exam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "36b0e076-8d1c-4ffb-af57-4daf6afed52c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import tqdm\n",
    "\n",
    "def generate_questions_parallel(base, query, question_type, question_nbr, ollama_model, difficulty):\n",
    "    \"\"\"\n",
    "    Generate questions in parallel to speed up the process.\n",
    "    \"\"\"\n",
    "    def generate_question(content):\n",
    "        if question_type.lower() == \"mcq\":\n",
    "            return {\n",
    "                \"type\": \"mcq\",\n",
    "                \"source_content\": content,\n",
    "                \"question_data\": generate_mcq(content, query, difficulty)\n",
    "            }\n",
    "        elif question_type.lower() == \"open-ended\":\n",
    "            return {\n",
    "                \"type\": \"open-ended\",\n",
    "                \"source_content\": content,\n",
    "                \"question_data\": generate_open_ended(content, query, difficulty)\n",
    "            }\n",
    "        else:\n",
    "            raise ValueError(\"Invalid question type. Use 'mcq' or 'open-ended'.\")\n",
    "\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        questions = list(\n",
    "            tqdm.tqdm(\n",
    "                executor.map(generate_question, base[:question_nbr]),\n",
    "                total=question_nbr,\n",
    "                desc=\"Generating Questions\"\n",
    "            )\n",
    "        )\n",
    "    return questions\n",
    "\n",
    "def exam_pipeline(query, question_type, question_nbr, faiss_index, ollama_model, difficulty=\"intermediate\", k=25, top_n=5):\n",
    "    \"\"\"\n",
    "    Optimized exam generation pipeline with parallel question generation.\n",
    "    \"\"\"\n",
    "    print(\"Retrieving relevant documents...\")\n",
    "    re_ranked_docs = retrieve_and_rerank(query, faiss_index, k, top_n)\n",
    "\n",
    "    if not re_ranked_docs:\n",
    "        print(\"No documents found for the given query.\")\n",
    "        return {}\n",
    "\n",
    "    print(f\"Retrieved {len(re_ranked_docs)} documents.\")\n",
    "\n",
    "    print(\"Cleaning retrieved content...\")\n",
    "    cleaned_chunks = clean_relevant_chunks_for_question_generation(re_ranked_docs, min_chunk_length=30)\n",
    "    print(f\"Normalized to {len(cleaned_chunks)} valid chunks.\")\n",
    "\n",
    "    print(\"Generating questions...\")\n",
    "    exam = {\"questions\": generate_questions_parallel(cleaned_chunks, query, question_type, question_nbr, ollama_model, difficulty)}\n",
    "    \n",
    "    print(f\"Generated {len(exam['questions'])} questions.\")\n",
    "    return exam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ce48e974-755b-4f9c-aa64-e77331c9cb6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving relevant documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MTehno\\AppData\\Local\\Temp\\ipykernel_12116\\1237832411.py:14: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  retrieved_docs = retriever.get_relevant_documents(query)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved 5 documents.\n",
      "Cleaning retrieved content...\n",
      "Normalized to 5 valid chunks.\n",
      "Generating questions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Questions: 100%|██████████████████████████████████████████████████████████████| 2/2 [00:40<00:00, 20.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 2 questions.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"les requettes sql\"\n",
    "question_type = \"mcq\" # mcq or open-ended\n",
    "question_nbr = 2\n",
    "faiss_index = loaded_faiss_index  # Your FAISS index\n",
    "ollama_model = OllamaLLM(model=\"llama3.2\")\n",
    "difficulty = \"avancé\" # 'débutant', 'intermédiaire', 'avancé'\n",
    "\n",
    "exam = exam_pipeline(query, question_type, question_nbr, faiss_index, ollama_model, difficulty)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "423b027b",
   "metadata": {},
   "source": [
    "## Display the Exam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "68889635-cb53-4e2f-b4d3-ca069f60be80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1 (mcq):\n",
      "==================================================\n",
      "Context:\n",
      "# Systèmes de Gestion de Bases de Données, Vertigo/CNAM, Paris\n",
      "\n",
      "# Exemples de questions (requêtes) posées à la base\n",
      "\n",
      "- Insérer un employé nommé Jean\n",
      "- Augmenter Jean de 10%\n",
      "- Détruire Jean\n",
      "- Chercher les employés cadres\n",
      "- Chercher les employés du département comptabilité\n",
      "- Salaire moyen des employés comptables, avec deux enfants, nés avant 1960 et travaillant à Paris\n",
      "\n",
      "Les requêtes sont émises avec un langage de requêtes (SQL2, OQL, SQL3, XQUERY, etc.).\n",
      "--------------------------------------------------\n",
      "Question:\n",
      "```json\n",
      "{\n",
      "    \"question\": \"Quelle est la réduction en masse d'un employé du système de gestion de bases de données Vertigo/CNAM ?\",\n",
      "    \"options\": {\n",
      "        \"A\": \"Insérer un employé\",\n",
      "        \"B\": \"Augmenter un employé de 10%\",\n",
      "        \"C\": \"Détruire un employé\",\n",
      "        \"D\": \"Supprimer un employé\"\n",
      "    },\n",
      "    \"correct_answer\": \"D\",\n",
      "    \"explanation\": \"La réduction en masse d'un employé est synonyme de suppression. Dans le contexte du système de gestion de bases de données Vertigo/CNAM, la commande `DELETE FROM employés WHERE nom = 'Jean'` serait utilisée pour supprimer l'employé Jean.\"\n",
      "}\n",
      "```\n",
      "==================================================\n",
      "Question 2 (mcq):\n",
      "==================================================\n",
      "Context:\n",
      "# Vertigo/CNAM, Paris\n",
      "\n",
      "# Optimisation\n",
      "\n",
      "SQL est déclaratif. L’utilisateur :\n",
      "\n",
      "- indique ce qu’il veut obtenir.\n",
      "- n’indique pas comment l’obtenir.\n",
      "\n",
      "Donc le système fait le reste :\n",
      "\n",
      "- comprendre la requête: il traduit la requête en algèbre relationnelle\n",
      "- Choisir la meilleure stratégie d’évaluation. On obtient un plan d’exécution physique\n",
      "- Evaluer le plan d’exécution\n",
      "--------------------------------------------------\n",
      "Question:\n",
      "```json\n",
      "{\n",
      "  \"question\": \"Quel est l'avantage principal de l'utilisation de requêtes SQL déclaratives, comme indiqué ci-dessous ?\",\n",
      "  \"options\": {\n",
      "    \"A\": \"L'utilisateur doit spécifier les étapes à suivre pour obtenir le résultat souhaité.\",\n",
      "    \"B\": \"Le système fait toutes les étapes nécessaires pour obtenir le résultat souhaité.\",\n",
      "    \"C\": \"La réusibilité des requêtes est améliorée en raison de leur nature déclarative.\",\n",
      "    \"D\": \"Les requêtes sont toujours interprétées par l'intermédiaire d'un compilateur.\"\n",
      "  },\n",
      "  \"correct_answer\": \"B\",\n",
      "  \"explanation\": \"La question SQL déclarative permet à l'utilisateur d'indiquer ce qu'il veut obtenir, sans spécifier comment l'obtenir. Le système fait alors le reste, ce qui permet une meilleure flexibilité et réusabilité des requêtes.\"\n",
      "}\n",
      "```\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "def display_exam(exam):\n",
    "    \"\"\"\n",
    "    Display the exam questions in a structured format.\n",
    "    \n",
    "    Args:\n",
    "        exam (dict): The generated exam with questions.\n",
    "    \"\"\"\n",
    "    for i, question in enumerate(exam['questions']):\n",
    "        print(f\"Question {i + 1} ({question['type']}):\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        # Display source content (question context)\n",
    "        print(\"Context:\")\n",
    "        print(question['source_content'])\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        # Display the question\n",
    "        print(\"Question:\")\n",
    "        print(question['question_data'])\n",
    "        \n",
    "        print(\"=\" * 50)\n",
    "\n",
    "# Display the exam in a structured format\n",
    "display_exam(exam)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6134bfe8-095d-4f49-9a78-6bd3adf4b2b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d84bc0-725c-46f4-99ab-66d8a3719046",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f04286-2494-4b3f-bc60-d608ab8e7b1d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
